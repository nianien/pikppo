"""
TTS Synthesis: Azure Neural TTS per segment with episode-level caching.

Functions:
- synthesize_tts: Original function (concatenates to tts_en.wav)
- synthesize_tts_per_segment: New function (per-segment WAVs, no concatenation)
"""
import hashlib
import json
import os
import re
import shutil
import subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

from pikppo.schema.dub_manifest import DubManifest
from pikppo.schema.tts_report import TTSReport, TTSSegmentReport, TTSSegmentStatus

# For audio diagnostics
try:
    import numpy as np
    import soundfile as sf
    AUDIO_DIAGNOSTICS_AVAILABLE = True
except ImportError:
    AUDIO_DIAGNOSTICS_AVAILABLE = False

# Cache configuration
CACHE_ENGINE = "azure"
CACHE_ENGINE_VER = "v1"
CACHE_SAMPLE_RATE = 24000
CACHE_CHANNELS = 1
CACHE_FORMAT = "wav"


def _normalize_text(text: str) -> str:
    """
    Normalize text for cache key generation.
    - strip()
    - collapse consecutive whitespace to single space
    """
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    return text


def _generate_cache_key(
    text: str,
    voice_id: str,
    prosody: Dict[str, Any],
    language: str,
) -> str:
    """
    Generate cache key for a TTS segment.
    
    Args:
        text: Normalized text
        voice_id: Azure voice ID (e.g., "en-US-JennyNeural")
        prosody: Prosody settings (rate, pitch, style, etc.)
        language: Language code (e.g., "en-US")
        
    Returns:
        SHA256 hash (hex string)
    """
    # Normalize text
    text_norm = _normalize_text(text)
    
    # Build payload
    payload = {
        "engine": CACHE_ENGINE,
        "engine_ver": CACHE_ENGINE_VER,
        "voice": voice_id,
        "lang": language,
        "format": CACHE_FORMAT,
        "sample_rate": CACHE_SAMPLE_RATE,
        "channels": CACHE_CHANNELS,
        "prosody": {
            "rate": prosody.get("rate", 1.0),
            "pitch": prosody.get("pitch", 0),
            "style": prosody.get("style", "general"),
            "role": prosody.get("role", ""),
            "volume": prosody.get("volume", ""),
        },
        "text": text_norm,
    }
    
    # Generate SHA256 hash
    payload_json = json.dumps(payload, sort_keys=True, ensure_ascii=False)
    cache_key = hashlib.sha256(payload_json.encode('utf-8')).hexdigest()
    
    return cache_key


def _get_cache_paths(output_dir: Path) -> tuple[Path, Path]:
    """
    Get cache directory and manifest path.
    
    Returns:
        (cache_dir, manifest_path)
    """
    cache_dir = output_dir / CACHE_ENGINE
    cache_dir.mkdir(parents=True, exist_ok=True)
    manifest_path = cache_dir / "manifest.jsonl"
    return cache_dir, manifest_path


def _write_cache_atomic(cache_file: Path, source_file: Path):
    """
    Atomically write cache file (write to .tmp first, then rename).
    
    Args:
        cache_file: Final cache file path
        source_file: Source file to copy
    """
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    temp_file = cache_file.with_suffix('.tmp')
    
    # Copy to temp file
    shutil.copy2(source_file, temp_file)
    
    # Atomic rename
    temp_file.replace(cache_file)


def _append_manifest(
    manifest_path: Path,
    seg_id: int,
    cache_key: str,
    voice_id: str,
    text: str,
):
    """
    Append entry to manifest.jsonl.
    
    Args:
        manifest_path: Path to manifest.jsonl
        seg_id: Segment ID
        cache_key: Cache key (SHA256)
        voice_id: Voice ID
        text: Original text (for debugging)
    """
    manifest_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Generate text digest
    text_digest = hashlib.sha1(text.encode('utf-8')).hexdigest()
    
    entry = {
        "seg": seg_id,
        "key": cache_key,
        "voice": voice_id,
        "text_sha1": text_digest,
        "ts": datetime.now().isoformat(),
    }
    
    with open(manifest_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")


def _normalize_audio_format(
    input_path: str,
    output_path: str,
    sample_rate: int = CACHE_SAMPLE_RATE,
    channels: int = CACHE_CHANNELS,
):
    """
    Normalize audio to specified format using ffmpeg.
    
    Args:
        input_path: Input audio file
        output_path: Output audio file
        sample_rate: Target sample rate
        channels: Target channels (1=mono, 2=stereo)
    """
    cmd = [
        "ffmpeg",
        "-i", input_path,
        "-ar", str(sample_rate),
        "-ac", str(channels),
        "-sample_fmt", "s16",  # 16-bit PCM
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True, capture_output=True)


def synthesize_tts(
    en_segments_path: str,
    voice_assignment_path: str,
    voice_pool_path: Optional[str],
    output_dir: str,
    *,
    azure_key: str,
    azure_region: str,
    language: str = "en-US",
    max_workers: int = 4,
) -> str:
    """
    Synthesize TTS for each segment using Azure Neural TTS with episode-level caching.
    
    Args:
        en_segments_path: Path to segments JSON file (ä¸´æ—¶æ–‡ä»¶ï¼Œç”± processor åˆ›å»º)
        voice_assignment_path: Path to voice_assignment.json
        voice_pool_path: Path to voice pool JSON (None = use default)
        output_dir: Output directory (should be .temp/tts)
        azure_key: Azure Speech Service key
        azure_region: Azure Speech Service region
        language: TTS language
        max_workers: Number of concurrent workers (not used in v1, kept for compatibility)
        
    Returns:
        Path to tts_en.wav
    """
    try:
        import azure.cognitiveservices.speech as speechsdk
    except ImportError:
        raise ImportError(
            "azure-cognitiveservices-speech is not installed. "
            "Install it with: pip install azure-cognitiveservices-speech"
        )
    
    from pikppo.models.voice_pool import VoicePool
    
    # Load data
    with open(en_segments_path, "r", encoding="utf-8") as f:
        en_segments = json.load(f)
    
    with open(voice_assignment_path, "r", encoding="utf-8") as f:
        voice_assignment = json.load(f)
    
    voice_pool = VoicePool(pool_path=voice_pool_path)
    
    # Initialize Azure Speech
    speech_config = speechsdk.SpeechConfig(
        subscription=azure_key,
        region=azure_region,
    )
    speech_config.speech_synthesis_language = language
    
    # Set output format to 24kHz mono PCM (WAV)
    # Note: Azure SDK doesn't directly support WAV, so we'll convert from MP3
    speech_config.set_speech_synthesis_output_format(
        speechsdk.SpeechSynthesisOutputFormat.Audio24Khz48KBitRateMonoMp3
    )
    
    output = Path(output_dir)
    # ä¿å­˜åˆ° .temp/tts/azure/segments ç›®å½•
    segments_dir = output / "azure" / "segments"
    segments_dir.mkdir(parents=True, exist_ok=True)
    
    # Get cache paths
    cache_dir, manifest_path = _get_cache_paths(output)
    
    # v1 æ•´æ”¹ï¼šåˆæˆæ¯ä¸ª segment åç«‹åˆ»åšæ®µå†…å¯¹é½ï¼Œåœ¨ concat å‰æ’å…¥ gap é™éŸ³æ®µ
    # æ’åº segments æŒ‰ start æ—¶é—´
    en_segments_sorted = sorted(en_segments, key=lambda x: x["start"])
    
    segment_files = []
    cache_hits = 0
    cache_misses = 0
    
    # ç»Ÿè®¡ä¿¡æ¯
    speedup_stats = []  # è®°å½•æ¯ä¸ª segment çš„ speedup
    compression_type_counts = {}  # è®°å½•å‹ç¼©ç±»å‹åˆ†å¸ƒ
    
    for i, seg in enumerate(en_segments_sorted):
        seg_id = seg['id']
        speaker = seg["speaker"]
        # æ”¯æŒä¸¤ç§å­—æ®µåï¼šen_textï¼ˆå‘åå…¼å®¹ï¼‰å’Œ textï¼ˆæ–°æ ¼å¼ï¼‰
        text = seg.get("en_text", seg.get("text", "")).strip()
        seg_start = seg["start"]
        seg_end = seg["end"]
        seg_duration = seg_end - seg_start
        duration_ms = seg_duration * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # è¯Šæ–­ä¿¡æ¯ 1: åˆæˆå‰æ£€æŸ¥
        print(f"[TTS DIAG] seg_id={seg_id}, start_ms={seg_start*1000:.0f}, end_ms={seg_end*1000:.0f}, duration_ms={duration_ms:.0f}")
        print(f"[TTS DIAG] TEXT: {repr(text)}")
        print(f"[TTS DIAG] DURATION_MS: {duration_ms:.1f}")
        
        # æ–­è¨€ï¼šæ–‡æœ¬ä¸èƒ½ä¸ºç©º
        assert text.strip(), f"Empty text for segment {seg_id}"
        
        if not text:
            # Empty segment - create silent audio of exact duration
            segment_file = segments_dir / f"seg_{seg_id:04d}.wav"
            _create_silent_audio(str(segment_file), seg_duration)
            segment_files.append((str(segment_file), seg_start, seg_end))
            continue
        
        voice_info = voice_assignment["speakers"].get(speaker, {})
        voice_id = voice_info.get("voice", {}).get("voice_id", "en-US-JennyNeural")
        
        # Get voice config from pool
        pool_key = voice_info.get("voice", {}).get("pool_key")
        if pool_key:
            voice_config = voice_pool.get_voice(pool_key)
            prosody = voice_config.get("prosody", {})
        else:
            prosody = {}
        
        # Generate cache key
        cache_key = _generate_cache_key(text, voice_id, prosody, language)
        cache_file = cache_dir / f"{cache_key}.wav"
        segment_file_raw = segments_dir / f"seg_{seg_id:04d}_raw.wav"
        segment_file = segments_dir / f"seg_{seg_id:04d}.wav"
        
        # Check cache
        if cache_file.exists():
            # Cache hit - copy to raw segment file
            shutil.copy2(cache_file, segment_file_raw)
            cache_hits += 1
            print(f"  ğŸ’¾ [{seg_id}] Cache hit: {text[:50]}...")
            
            # è¯Šæ–­ä¿¡æ¯ 2a: ç¼“å­˜å‘½ä¸­åä¹Ÿæ£€æŸ¥éŸ³é¢‘
            if AUDIO_DIAGNOSTICS_AVAILABLE and segment_file_raw.exists():
                try:
                    audio, sr = sf.read(str(segment_file_raw))
                    rms = np.sqrt(np.mean(audio**2)) if len(audio) > 0 else 0.0
                    print(f"[TTS DIAG] AUDIO (from cache): dtype={audio.dtype}, shape={audio.shape}, min={audio.min():.6f}, max={audio.max():.6f}, RMS={rms:.6f}")
                except Exception as e:
                    print(f"[TTS DIAG] Failed to read cached audio for diagnostics: {e}")
        else:
            # Cache miss - synthesize
            cache_misses += 1
            
            # Set voice
            speech_config.speech_synthesis_voice_name = voice_id
            
            # Create temporary file for Azure output (Azure outputs MP3 by default)
            temp_azure_output = segments_dir / f"seg_{seg_id:04d}_azure.mp3"
            audio_config = speechsdk.audio.AudioOutputConfig(filename=str(temp_azure_output))
            
            synthesizer = speechsdk.SpeechSynthesizer(
                speech_config=speech_config,
                audio_config=audio_config,
            )
            
            # Build SSML with prosody
            ssml = f"""<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="{language}">
    <voice name="{voice_id}">
        <prosody rate="{prosody.get('rate', 1.0)}" pitch="{prosody.get('pitch', 0)}%">
            {text}
        </prosody>
    </voice>
</speak>"""
            
            try:
                result = synthesizer.speak_ssml_async(ssml).get()
                
                if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                    # Normalize to WAV 24k mono PCM
                    _normalize_audio_format(
                        str(temp_azure_output),
                        str(segment_file_raw),
                        sample_rate=CACHE_SAMPLE_RATE,
                        channels=CACHE_CHANNELS,
                    )
                    
                    # è¯Šæ–­ä¿¡æ¯ 2: normalize åæ£€æŸ¥éŸ³é¢‘
                    if AUDIO_DIAGNOSTICS_AVAILABLE and segment_file_raw.exists():
                        try:
                            audio, sr = sf.read(str(segment_file_raw))
                            rms = np.sqrt(np.mean(audio**2)) if len(audio) > 0 else 0.0
                            print(f"[TTS DIAG] AUDIO (after normalize): dtype={audio.dtype}, shape={audio.shape}, min={audio.min():.6f}, max={audio.max():.6f}, RMS={rms:.6f}")
                        except Exception as e:
                            print(f"[TTS DIAG] Failed to read audio for diagnostics: {e}")
                    
                    # Write to cache (atomic)
                    _write_cache_atomic(cache_file, segment_file_raw)
                    
                    # Clean up temp Azure output
                    temp_azure_output.unlink(missing_ok=True)
                else:
                    cancellation_details = speechsdk.CancellationDetails(result)
                    error_msg = f"{result.reason}"
                    if cancellation_details.reason == speechsdk.CancellationReason.Error:
                        error_msg += f": {cancellation_details.error_details}"
                    print(f"Warning: TTS failed for segment {seg_id}: {error_msg}")
                    # Create silent segment as fallback
                    _create_silent_audio(str(segment_file_raw), seg_duration)
                    temp_azure_output.unlink(missing_ok=True)
            except Exception as e:
                print(f"Warning: TTS exception for segment {seg_id}: {e}")
                # Create silent segment as fallback
                _create_silent_audio(str(segment_file_raw), seg_duration)
                temp_azure_output.unlink(missing_ok=True)
        
        # v1 æ•´æ”¹ï¼šåˆæˆæ¯ä¸ª segment åç«‹åˆ»åšæ®µå†…å¯¹é½
        # è·å–ä¸‹ä¸€å¥çš„ä¿¡æ¯ï¼ˆç”¨äºæ£€æŸ¥é‡å ï¼‰
        next_seg = en_segments_sorted[i + 1] if i + 1 < len(en_segments_sorted) else None
        next_seg_start_ms = next_seg.get("start") * 1000.0 if next_seg else None
        current_seg_start_ms = seg_start * 1000.0
        
        # ç»Ÿè®¡ä¿¡æ¯
        seg_stats: Dict[str, Any] = {}
        
        _align_segment_to_window(
            str(segment_file_raw),
            str(segment_file),
            duration_ms,  # budget_msï¼ˆæ¯«ç§’ï¼‰
            text=text,
            next_seg_start_ms=next_seg_start_ms,
            current_seg_start_ms=current_seg_start_ms,
            stats=seg_stats,
        )
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        if "speedup" in seg_stats:
            speedup_stats.append(seg_stats)
            comp_type = seg_stats.get("compression_type", "unknown")
            compression_type_counts[comp_type] = compression_type_counts.get(comp_type, 0) + 1
        
        # æ‰“å°æ¯ä¸ª segment çš„è¯¦ç»†ç»Ÿè®¡ï¼ˆraw_duration / trimmed_duration / final_durationï¼‰
        if "original_duration_ms" in seg_stats:
            original_ms = seg_stats["original_duration_ms"]
            trimmed_ms = seg_stats.get("trimmed_duration_ms", original_ms)
            final_ms = duration_ms  # budget_ms
            print(f"  ğŸ“ [{seg_id}] Duration: {original_ms:.0f}ms (raw) -> {trimmed_ms:.0f}ms (trimmed) -> {final_ms:.0f}ms (final)")
        
        # è¯Šæ–­ä¿¡æ¯ 3: align åæ£€æŸ¥éŸ³é¢‘
        if AUDIO_DIAGNOSTICS_AVAILABLE and segment_file.exists():
            try:
                audio, sr = sf.read(str(segment_file))
                rms = np.sqrt(np.mean(audio**2)) if len(audio) > 0 else 0.0
                actual_duration = len(audio) / sr if sr > 0 else 0.0
                print(f"[TTS DIAG] AUDIO (after align): dtype={audio.dtype}, shape={audio.shape}, duration={actual_duration:.3f}s, min={audio.min():.6f}, max={audio.max():.6f}, RMS={rms:.6f}")
            except Exception as e:
                print(f"[TTS DIAG] Failed to read aligned audio for diagnostics: {e}")
        
        segment_files.append((str(segment_file), seg_start, seg_end))
        
        # Append to manifest
        _append_manifest(manifest_path, seg_id, cache_key, voice_id, text)
    
    # Print cache statistics
    total = cache_hits + cache_misses
    if total > 0:
        hit_rate = (cache_hits / total) * 100
        print(f"  ğŸ“Š Cache: {cache_hits}/{total} hits ({hit_rate:.1f}%)")
    
    # Print compression statistics
    if speedup_stats and AUDIO_DIAGNOSTICS_AVAILABLE:
        # æå– speedup å€¼
        speedup_values = [s.get("speedup", 1.0) if isinstance(s, dict) else s for s in speedup_stats]
        speedup_array = np.array(speedup_values)
        p50 = np.percentile(speedup_array, 50)
        p90 = np.percentile(speedup_array, 90)
        p99 = np.percentile(speedup_array, 99)
        print(f"  ğŸ“Š Speedup stats: P50={p50:.2f}Ã—, P90={p90:.2f}Ã—, P99={p99:.2f}Ã—")
        print(f"  ğŸ“Š Compression types: {compression_type_counts}")
        
        aggressive_count = compression_type_counts.get("aggressive", 0) + compression_type_counts.get("aggressive_max", 0)
        aggressive_pct = (aggressive_count / len(speedup_stats)) * 100 if speedup_stats else 0
        if aggressive_pct > 5:
            print(f"  âš ï¸  Warning: Aggressive compression rate is {aggressive_pct:.1f}% (>5%), consider adjusting upstream (segmentation/TTS speed)")
        
        # Print silence trimming statistics
        total_saved_ms = sum(s.get("silence_saved_ms", 0) for s in speedup_stats if isinstance(s, dict))
        if total_saved_ms > 0:
            avg_saved_ms = total_saved_ms / len(speedup_stats)
            print(f"  ğŸ“Š Silence trimming: avg saved {avg_saved_ms:.0f}ms per segment (total {total_saved_ms:.0f}ms saved)")
    
    # v1 æ•´æ”¹ï¼šåœ¨ concat å‰æ’å…¥ gap é™éŸ³æ®µ
    tts_output = output / "tts_en.wav"
    _concatenate_with_gaps(segment_files, str(tts_output))
    
    return str(tts_output)


def _create_silent_audio(output_path: str, duration: float):
    """Create silent audio file of specified duration in WAV 24k mono PCM format."""
    cmd = [
        "ffmpeg",
        "-f", "lavfi",
        "-i", f"anullsrc=r={CACHE_SAMPLE_RATE}:cl=mono",
        "-t", str(duration),
        "-ar", str(CACHE_SAMPLE_RATE),
        "-ac", str(CACHE_CHANNELS),
        "-sample_fmt", "s16",
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True, capture_output=True)


def _allow_aggressive_compression(text: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦å…è®¸ä½¿ç”¨ aggressive å‹ç¼©ï¼ˆ>1.25Ã—ï¼‰ã€‚
    
    å…è®¸æ¡ä»¶ï¼ˆçŸ­è¯/æ‹Ÿå£°è¯ï¼‰ï¼š
    - word_count <= 3
    - åŒ…å«æ‹Ÿå£°è¯ï¼ˆha/haha/ah/ohï¼‰
    - çŸ­å£å¤´è¯­
    
    Args:
        text: æ–‡æœ¬å†…å®¹
    
    Returns:
        True å¦‚æœå…è®¸ aggressive å‹ç¼©
    """
    if not text:
        return False
    
    # è®¡ç®—å•è¯æ•°
    words = text.split()
    word_count = len(words)
    
    # æ¡ä»¶ 1: å•è¯æ•° <= 3
    if word_count <= 3:
        return True
    
    # æ¡ä»¶ 2: åŒ…å«æ‹Ÿå£°è¯/ç¬‘å£°
    text_lower = text.lower()
    interjections = ["ha", "haha", "ah", "oh", "hey", "bro", "wow", "yeah", "ok", "okay"]
    for interj in interjections:
        if interj in text_lower:
            return True
    
    return False


def _trim_silence(
    input_path: str,
    output_path: str,
    threshold_db: float = -40.0,
    min_silence_ms: float = 50.0,
) -> tuple[float, float]:
    """
    å»é™¤éŸ³é¢‘é¦–å°¾é™éŸ³ï¼ˆåªè£é™éŸ³ï¼Œä¸è£è¯­éŸ³ï¼‰ã€‚
    
    é‡è¦ï¼šè¿™æ˜¯ç¬¬ä¸€æ­¥ï¼Œå¿…é¡»åœ¨åˆ¤æ–­æ˜¯å¦è¶…é•¿ä¹‹å‰æ‰§è¡Œã€‚
    
    ç­–ç•¥ï¼š
    - åªè£é¦–å°¾è¿ç»­é™éŸ³ï¼Œä¸åŠ¨ä¸­é—´åœé¡¿
    - é˜ˆå€¼ï¼š-40 dBFSï¼ˆå·¥ç¨‹å®‰å…¨æ ‡å‡†ï¼‰
    - æœ€å°è¿ç»­é™éŸ³é•¿åº¦ï¼š50 msï¼ˆé¿å…è¯¯è£ï¼‰
    
    Args:
        input_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶
        output_path: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶ï¼ˆå»é™¤é™éŸ³åï¼‰
        threshold_db: é™éŸ³é˜ˆå€¼ï¼ˆdBFSï¼‰ï¼Œé»˜è®¤ -40 dB
        min_silence_ms: æœ€å°è¿ç»­é™éŸ³é•¿åº¦ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤ 50 ms
    
    Returns:
        (trimmed_duration_sec, saved_ms) å…ƒç»„ï¼š
        - trimmed_duration_sec: å»é™¤é™éŸ³åçš„å®é™…æ—¶é•¿ï¼ˆç§’ï¼‰
        - saved_ms: èŠ‚çœçš„æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
    """
    import subprocess
    
    # è·å–åŸå§‹æ—¶é•¿
    result = subprocess.run(
        ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", input_path],
        capture_output=True,
        text=True,
        check=True,
    )
    duration_str = result.stdout.strip()
    if duration_str == "N/A" or not duration_str:
        # å¦‚æœ ffprobe è¿”å› N/A æˆ–ç©ºï¼Œè¯´æ˜æ–‡ä»¶æ— æ•ˆï¼Œè¿”å›é»˜è®¤å€¼
        original_duration = 0.0
    else:
        original_duration = float(duration_str)
    
    # ä½¿ç”¨ silenceremove å»é™¤é¦–å°¾é™éŸ³
    # start_periods=1: åªæ£€æµ‹å¼€å¤´çš„ä¸€æ®µé™éŸ³
    # stop_periods=1: åªæ£€æµ‹ç»“å°¾çš„ä¸€æ®µé™éŸ³
    # start_duration / stop_duration: æœ€å°è¿ç»­é™éŸ³é•¿åº¦ï¼ˆç§’ï¼‰
    # start_threshold / stop_threshold: é™éŸ³é˜ˆå€¼ï¼ˆdBï¼‰
    # detection=peak: ä½¿ç”¨å³°å€¼æ£€æµ‹ï¼ˆæ›´å‡†ç¡®ï¼‰
    min_silence_sec = min_silence_ms / 1000.0
    filter_str = (
        f"silenceremove="
        f"start_periods=1:"
        f"start_duration={min_silence_sec}:"
        f"start_threshold={threshold_db}dB:"
        f"detection=peak:"
        f"stop_periods=-1:"
        f"stop_duration={min_silence_sec}:"
        f"stop_threshold={threshold_db}dB"
    )
    
    cmd = [
        "ffmpeg",
        "-i", input_path,
        "-af", filter_str,
        "-ar", str(CACHE_SAMPLE_RATE),
        "-ac", str(CACHE_CHANNELS),
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True, capture_output=True)
    
    # è·å–å»é™¤é™éŸ³åçš„æ—¶é•¿
    result = subprocess.run(
        ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", output_path],
        capture_output=True,
        text=True,
        check=True,
    )
    duration_str = result.stdout.strip()
    if duration_str == "N/A" or not duration_str:
        # å¦‚æœ ffprobe è¿”å› N/A æˆ–ç©ºï¼Œè¯´æ˜æ–‡ä»¶æ— æ•ˆï¼Œä½¿ç”¨åŸå§‹æ—¶é•¿
        trimmed_duration = original_duration
    else:
        trimmed_duration = float(duration_str)
    
    saved_ms = (original_duration - trimmed_duration) * 1000.0
    
    return trimmed_duration, saved_ms


def _align_segment_to_window(
    input_path: str,
    output_path: str,
    budget_ms: float,  # ç›®æ ‡æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
    text: Optional[str] = None,
    next_seg_start_ms: Optional[float] = None,
    current_seg_start_ms: Optional[float] = None,
    stats: Optional[Dict[str, Any]] = None,  # ç”¨äºç»Ÿè®¡
):
    """
    å°†éŸ³é¢‘å¯¹é½åˆ°æ—¶é—´çª—å£ï¼ˆåˆ†çº§å‹ç¼© + å…è®¸æº¢å‡º + æœ€åæˆªæ–­ï¼‰ã€‚
    
    å†³ç­–é¡ºåºï¼š
    1. Trim é™éŸ³
    2. å¦‚æœ real_ms <= budget_ms â†’ padding
    3. å¦‚æœè¶…çª—ä½†ä¸é‡å  â†’ æ”¾è¡Œï¼ˆä¸å‹ç¼©ï¼Œä¸æˆªæ–­ï¼‰
    4. éœ€è¦å‹ç¼©æ—¶ï¼šsafe (1.25Ã—) â†’ aggressive (1.6Ã— æˆ– 2Ã—ï¼Œéœ€è§¦å‘æ¡ä»¶)
    5. å‹åˆ°æé™ä»ä¸å¤Ÿ â†’ æˆªæ–­ï¼ˆåŠ æ·¡å‡ºï¼‰
    
    Args:
        input_path: åŸå§‹éŸ³é¢‘æ–‡ä»¶
        output_path: å¯¹é½åçš„éŸ³é¢‘æ–‡ä»¶
        budget_ms: ç›®æ ‡æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        text: æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦å…è®¸ aggressive å‹ç¼©ï¼‰
        next_seg_start_ms: ä¸‹ä¸€å¥çš„å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºæ£€æŸ¥é‡å 
        current_seg_start_ms: å½“å‰å¥çš„å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºæ£€æŸ¥é‡å 
        stats: ç»Ÿè®¡å­—å…¸ï¼ˆç”¨äºè®°å½• speedup ç­‰ä¿¡æ¯ï¼‰
    """
    import subprocess
    
    budget_sec = budget_ms / 1000.0
    
    # Step 0: Trim é™éŸ³ï¼ˆå¿…é¡»æ˜¯ç¬¬ä¸€æ­¥ï¼Œåœ¨åˆ¤æ–­æ˜¯å¦è¶…é•¿ä¹‹å‰ï¼‰
    # æ³¨æ„ï¼šå¦‚æœåŸå§‹éŸ³é¢‘æ¯” budget çŸ­ï¼Œä¸éœ€è¦ trim é™éŸ³ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹éŸ³é¢‘
    # å› ä¸º trim å¯èƒ½ä¼šè¿‡åº¦è£å‰ªï¼Œå¯¼è‡´éŸ³é¢‘å†…å®¹ä¸¢å¤±
    temp_trimmed = input_path + ".trimmed.wav"
    
    # å…ˆæ£€æŸ¥åŸå§‹éŸ³é¢‘æ—¶é•¿
    result = subprocess.run(
        ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", input_path],
        capture_output=True,
        text=True,
        check=True,
    )
    duration_str = result.stdout.strip()
    if duration_str == "N/A" or not duration_str:
        original_duration_sec = 0.0
    else:
        original_duration_sec = float(duration_str)
    original_ms = original_duration_sec * 1000.0
    
    # å¦‚æœåŸå§‹éŸ³é¢‘æ¯” budget çŸ­ï¼Œä¸ trim é™éŸ³ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹éŸ³é¢‘
    if original_ms <= budget_ms:
        # åŸå§‹éŸ³é¢‘æ¯” budget çŸ­ï¼Œä¸éœ€è¦ trimï¼Œç›´æ¥ä½¿ç”¨åŸå§‹éŸ³é¢‘
        real_sec = original_duration_sec
        real_ms = original_ms
        saved_ms = 0.0
        # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶ï¼Œä¸éœ€è¦ trim
        shutil.copy2(input_path, temp_trimmed)
    else:
        # åŸå§‹éŸ³é¢‘æ¯” budget é•¿ï¼Œéœ€è¦ trim é™éŸ³
        real_sec, saved_ms = _trim_silence(input_path, temp_trimmed)
        real_ms = real_sec * 1000.0
    
    # è®°å½•åŸå§‹æ—¶é•¿ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
    result = subprocess.run(
        ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", input_path],
        capture_output=True,
        text=True,
        check=True,
    )
    duration_str = result.stdout.strip()
    if duration_str == "N/A" or not duration_str:
        # å¦‚æœ ffprobe è¿”å› N/A æˆ–ç©ºï¼Œè¯´æ˜æ–‡ä»¶æ— æ•ˆï¼Œä½¿ç”¨ trimmed æ—¶é•¿
        original_duration_sec = real_sec
    else:
        original_duration_sec = float(duration_str)
    original_ms = original_duration_sec * 1000.0
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if saved_ms > 10:  # åªæ‰“å°æœ‰æ„ä¹‰çš„èŠ‚çœ
        print(f"  âœ‚ï¸  Trimmed silence: {original_ms:.0f}ms -> {real_ms:.0f}ms (saved {saved_ms:.0f}ms)")
    
    if stats is not None:
        stats["original_duration_ms"] = original_ms
        stats["trimmed_duration_ms"] = real_ms
        stats["silence_saved_ms"] = saved_ms
    
    # è®¡ç®—éœ€è¦çš„å‹ç¼©æ¯”ï¼ˆratio < 1 è¡¨ç¤ºéœ€è¦åŠ é€Ÿï¼‰
    ratio = budget_ms / real_ms if real_ms > 0 else 1.0
    speedup = 1.0 / ratio if ratio > 0 else 1.0  # speedup > 1 è¡¨ç¤ºåŠ é€Ÿ
    
    # Step 1: å¦‚æœ real_ms <= budget_ms â†’ padding
    if real_ms <= budget_ms:
        pad_duration = budget_ms - real_ms
        # æ³¨æ„ï¼šä¸è¦ä½¿ç”¨ -t å‚æ•°ï¼Œå› ä¸º apad ä¼šè‡ªåŠ¨ padding åˆ°æŒ‡å®šæ—¶é•¿
        # ä½¿ç”¨ -t ä¼šå¼ºåˆ¶æˆªæ–­ï¼Œå¯¼è‡´åŸå§‹éŸ³é¢‘è¢«æˆªæ–­
        cmd = [
            "ffmpeg",
            "-i", temp_trimmed,
            "-af", f"apad=pad_dur={pad_duration/1000.0}",
            "-ar", str(CACHE_SAMPLE_RATE),
            "-ac", str(CACHE_CHANNELS),
            "-y",
            output_path,
        ]
        subprocess.run(cmd, check=True, capture_output=True)
        Path(temp_trimmed).unlink(missing_ok=True)
        if stats is not None:
            stats["speedup"] = 1.0
            stats["compression_type"] = "padding"
        return
    
    # Step 2: å¦‚æœè¶…çª—ä½†ä¸é‡å  â†’ æ”¾è¡Œï¼ˆä¸å‹ç¼©ï¼Œä¸æˆªæ–­ï¼‰
    if next_seg_start_ms is not None and current_seg_start_ms is not None:
        # è®¡ç®—å½“å‰å¥çš„å®é™…ç»“æŸæ—¶é—´ï¼ˆå¦‚æœæ”¾è¡Œï¼‰
        actual_end_ms = current_seg_start_ms + real_ms
        if actual_end_ms <= next_seg_start_ms:
            # ä¸é‡å ï¼Œç›´æ¥æ”¾è¡Œï¼ˆä¿ç•™å®Œæ•´éŸ³é¢‘ï¼Œä¸æˆªæ–­ï¼‰
            print(f"  âœ… Overflow allowed: {real_ms:.0f}ms > {budget_ms:.0f}ms, but no overlap with next segment (ends at {actual_end_ms:.0f}ms, next starts at {next_seg_start_ms:.0f}ms)")
            shutil.copy2(temp_trimmed, output_path)
            Path(temp_trimmed).unlink(missing_ok=True)
            if stats is not None:
                stats["speedup"] = 1.0
                stats["compression_type"] = "overflow_allowed"
            return
        else:
            # ä¼šé‡å ï¼Œéœ€è¦å¤„ç†
            overlap_ms = actual_end_ms - next_seg_start_ms
            print(f"  âš ï¸  Would overlap: {real_ms:.0f}ms > {budget_ms:.0f}ms, would overlap by {overlap_ms:.0f}ms (ends at {actual_end_ms:.0f}ms, next starts at {next_seg_start_ms:.0f}ms)")
    
    # Step 3: éœ€è¦å‹ç¼©æ—¶ï¼Œåˆ†çº§å¤„ç†
    # safe: ratio >= 0.80 (speedup <= 1.25Ã—)
    # aggressive: ratio >= 0.625 (speedup <= 1.6Ã—) æˆ– ratio >= 0.50 (speedup <= 2Ã—ï¼Œéœ€è§¦å‘æ¡ä»¶)
    
    if ratio >= 0.80:
        # Safe å‹ç¼©ï¼ˆ1.25Ã— ä»¥å†…ï¼‰
        compression_ratio = ratio
        compression_type = "safe"
    elif ratio >= 0.625:
        # å¯ä»¥å°è¯• 1.6Ã—
        compression_ratio = ratio
        compression_type = "moderate"
    elif ratio >= 0.50 and _allow_aggressive_compression(text or ""):
        # Aggressive å‹ç¼©ï¼ˆ2Ã—ï¼Œä½†éœ€è¦è§¦å‘æ¡ä»¶ï¼‰
        compression_ratio = ratio
        compression_type = "aggressive"
    else:
        # å‹åˆ°æé™ä»ä¸å¤Ÿ â†’ æˆªæ–­ï¼ˆåŠ æ·¡å‡ºï¼‰
        # å…ˆå°è¯•æœ€å¤§å…è®¸çš„å‹ç¼©ï¼ˆå¦‚æœä¸å…è®¸ aggressiveï¼Œåˆ™ç”¨ 1.6Ã—ï¼‰
        if _allow_aggressive_compression(text or ""):
            compression_ratio = 0.50  # 2Ã—
            compression_type = "aggressive_max"
        else:
            compression_ratio = 0.625  # 1.6Ã—
            compression_type = "moderate_max"
        
        # åº”ç”¨å‹ç¼©åå¦‚æœä»è¶…çª—ï¼Œåˆ™æˆªæ–­
        compressed_duration = real_sec * compression_ratio
        if compressed_duration > budget_sec:
            # éœ€è¦æˆªæ–­ï¼ˆåŠ æ·¡å‡ºï¼‰
            cmd = [
                "ffmpeg",
                "-i", temp_trimmed,
                "-af", f"atempo={1.0/compression_ratio},afade=t=out:st={budget_sec-0.1}:d=0.1",
                "-t", str(budget_sec),
                "-ar", str(CACHE_SAMPLE_RATE),
                "-ac", str(CACHE_CHANNELS),
                "-y",
                output_path,
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            Path(temp_trimmed).unlink(missing_ok=True)
            if stats is not None:
                stats["speedup"] = 1.0 / compression_ratio
                stats["compression_type"] = "hard_cut"
            print(f"Warning: Segment too long ({real_ms:.0f}ms > {budget_ms:.0f}ms), hard cut with fade")
            return
    
    # åº”ç”¨å‹ç¼©
    speedup_ratio = 1.0 / compression_ratio
    # atempo supports 0.5-2.0, chain for larger ratios
    if speedup_ratio < 0.5:
        ratios = []
        remaining = speedup_ratio
        while remaining < 0.5:
            ratios.append(0.5)
            remaining /= 0.5
        ratios.append(remaining)
        filter_str = ",".join([f"atempo={r}" for r in ratios])
    elif speedup_ratio > 2.0:
        ratios = []
        remaining = speedup_ratio
        while remaining > 2.0:
            ratios.append(2.0)
            remaining /= 2.0
        ratios.append(remaining)
        filter_str = ",".join([f"atempo={r}" for r in ratios])
    else:
        filter_str = f"atempo={speedup_ratio}"
    
    # è®¡ç®—å‹ç¼©åçš„å®é™…æ—¶é•¿
    compressed_duration_sec = real_sec * compression_ratio
    compressed_duration_ms = compressed_duration_sec * 1000.0
    
    # å¦‚æœå‹ç¼©åéŸ³é¢‘æ¯” budget çŸ­ï¼Œéœ€è¦ paddingï¼›å¦‚æœæ¯” budget é•¿ï¼Œéœ€è¦æˆªæ–­
    if compressed_duration_sec <= budget_sec:
        # å‹ç¼©åéŸ³é¢‘æ¯” budget çŸ­ï¼Œå…ˆå‹ç¼©ï¼Œç„¶å padding åˆ° budget
        pad_duration_ms = budget_ms - compressed_duration_ms
        filter_str_with_pad = f"{filter_str},apad=pad_dur={pad_duration_ms/1000.0}"
        final_duration = budget_sec
        cmd = [
            "ffmpeg",
            "-i", temp_trimmed,
            "-af", filter_str_with_pad,
            "-t", str(final_duration),
            "-ar", str(CACHE_SAMPLE_RATE),
            "-ac", str(CACHE_CHANNELS),
            "-y",
            output_path,
        ]
    else:
        # å‹ç¼©åéŸ³é¢‘ä»æ¯” budget é•¿ï¼Œéœ€è¦æˆªæ–­ï¼ˆåŠ æ·¡å‡ºï¼‰
        filter_str_with_fade = f"{filter_str},afade=t=out:st={budget_sec-0.1}:d=0.1"
        final_duration = budget_sec
        cmd = [
            "ffmpeg",
            "-i", temp_trimmed,
            "-af", filter_str_with_fade,
            "-t", str(final_duration),
            "-ar", str(CACHE_SAMPLE_RATE),
            "-ac", str(CACHE_CHANNELS),
            "-y",
            output_path,
        ]
    
    if compression_type in ["aggressive", "aggressive_max"]:
        print(f"Info: Segment compressed aggressively ({real_ms:.0f}ms -> {compressed_duration_ms:.0f}ms (compressed) -> {budget_ms:.0f}ms (final), {speedup_ratio:.2f}Ã— speed, type={compression_type})")
    
    subprocess.run(cmd, check=True, capture_output=True)
    Path(temp_trimmed).unlink(missing_ok=True)
    
    if stats is not None:
        stats["speedup"] = speedup_ratio
        stats["compression_type"] = compression_type


def _concatenate_with_gaps(segment_files: List[tuple], output_path: str):
    """
    Concatenate segments with gap silence inserted between them.

    v1 æ•´æ”¹ï¼šåœ¨ concat å‰æ’å…¥ gap é™éŸ³æ®µï¼ˆseg.start - prev.endï¼‰ã€‚

    Args:
        segment_files: List of (file_path, start_time, end_time) tuples, sorted by start_time
        output_path: Output audio file path
    """
    if not segment_files:
        raise ValueError("No segments to concatenate")

    concat_list = []
    prev_end = 0.0

    for file_path, seg_start, seg_end in segment_files:
        # Insert gap silence if needed (including first segment's leading silence)
        gap = seg_start - prev_end
        if gap > 0.01:  # Only insert if gap > 10ms
            gap_file = Path(file_path).parent / f"gap_{len(concat_list)}.wav"
            _create_silent_audio(str(gap_file), gap)
            concat_list.append(str(gap_file))

        # Add segment
        concat_list.append(file_path)
        prev_end = seg_end

    # v1 æ•´æ”¹ï¼šç¡®ä¿æ€»æ—¶é•¿æ­£ç¡®ï¼ˆåœ¨æœ€åè¡¥é™éŸ³åˆ°æœ€åä¸€ä¸ª segment çš„ endï¼‰
    # ä½†è¿™é‡Œä¸éœ€è¦ï¼Œå› ä¸º concat ä¼šè‡ªåŠ¨å¤„ç†æ€»æ—¶é•¿

    # Create concat file list
    concat_file = Path(output_path).parent / "concat_list.txt"
    with open(concat_file, "w") as f:
        for file in concat_list:
            f.write(f"file '{file}'\n")

    # Concatenate using ffmpeg
    cmd = [
        "ffmpeg",
        "-f", "concat",
        "-safe", "0",
        "-i", str(concat_file),
        "-c", "copy",
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True, capture_output=True)

    # Clean up
    concat_file.unlink()
    # Clean up gap files
    for file in concat_list:
        if "gap_" in file:
            Path(file).unlink(missing_ok=True)


def synthesize_tts_per_segment(
    dub_manifest: DubManifest,
    voice_assignment: Dict[str, Any],
    voice_pool_path: Optional[str],
    segments_dir: str,
    temp_dir: str,
    *,
    azure_key: str,
    azure_region: str,
    language: str = "en-US",
    max_workers: int = 4,
) -> TTSReport:
    """
    Per-segment TTS synthesis for Timeline-First Architecture.

    Each utterance in dub_manifest is synthesized to an individual WAV file.
    No concatenation is performed (that's handled by Mix phase).

    Args:
        dub_manifest: DubManifest object (SSOT for dubbing)
        voice_assignment: Speaker -> voice mapping
        voice_pool_path: Path to voice pool JSON
        segments_dir: Output directory for per-segment WAVs
        temp_dir: Temporary directory for intermediate files
        azure_key: Azure Speech Service key
        azure_region: Azure Speech Service region
        language: TTS language
        max_workers: Number of concurrent workers (not used in v1)

    Returns:
        TTSReport with per-segment synthesis results
    """
    try:
        import azure.cognitiveservices.speech as speechsdk
    except ImportError:
        raise ImportError(
            "azure-cognitiveservices-speech is not installed. "
            "Install it with: pip install azure-cognitiveservices-speech"
        )

    from pikppo.models.voice_pool import VoicePool

    voice_pool = VoicePool(pool_path=voice_pool_path)

    # Initialize Azure Speech
    speech_config = speechsdk.SpeechConfig(
        subscription=azure_key,
        region=azure_region,
    )
    speech_config.speech_synthesis_language = language
    speech_config.set_speech_synthesis_output_format(
        speechsdk.SpeechSynthesisOutputFormat.Audio24Khz48KBitRateMonoMp3
    )

    output_dir = Path(segments_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    temp_path = Path(temp_dir)
    temp_path.mkdir(parents=True, exist_ok=True)

    # Get cache paths
    cache_dir, manifest_path = _get_cache_paths(temp_path)

    segment_reports: List[TTSSegmentReport] = []

    for utt in dub_manifest.utterances:
        utt_id = utt.utt_id
        text = utt.text_en.strip()
        budget_ms = utt.budget_ms
        speaker = utt.speaker
        max_rate = utt.tts_policy.max_rate
        allow_extend_ms = utt.tts_policy.allow_extend_ms

        # Output file path
        segment_file = output_dir / f"seg_{utt_id}.wav"
        segment_file_raw = temp_path / f"seg_{utt_id}_raw.wav"

        if not text:
            # Empty text - create silent audio
            _create_silent_audio(str(segment_file), budget_ms / 1000.0)
            segment_reports.append(
                TTSSegmentReport(
                    utt_id=utt_id,
                    budget_ms=budget_ms,
                    raw_ms=0,
                    trimmed_ms=0,
                    final_ms=budget_ms,
                    rate=1.0,
                    status=TTSSegmentStatus.SUCCESS,
                    output_path=str(segment_file.relative_to(output_dir.parent)),
                )
            )
            continue

        # Get voice configuration
        voice_info = voice_assignment["speakers"].get(speaker, {})
        voice_id = voice_info.get("voice", {}).get("voice_id", "en-US-JennyNeural")
        pool_key = voice_info.get("voice", {}).get("pool_key")
        prosody = {}
        if pool_key:
            voice_config = voice_pool.get_voice(pool_key)
            prosody = voice_config.get("prosody", {})

        # Generate cache key
        cache_key = _generate_cache_key(text, voice_id, prosody, language)
        cache_file = cache_dir / f"{cache_key}.wav"

        try:
            # Check cache
            if cache_file.exists():
                shutil.copy2(cache_file, segment_file_raw)
                print(f"  ğŸ’¾ [{utt_id}] Cache hit")
            else:
                # Synthesize
                speech_config.speech_synthesis_voice_name = voice_id
                temp_azure_output = temp_path / f"seg_{utt_id}_azure.mp3"
                audio_config = speechsdk.audio.AudioOutputConfig(filename=str(temp_azure_output))

                synthesizer = speechsdk.SpeechSynthesizer(
                    speech_config=speech_config,
                    audio_config=audio_config,
                )

                ssml = f"""<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="{language}">
    <voice name="{voice_id}">
        <prosody rate="{prosody.get('rate', 1.0)}" pitch="{prosody.get('pitch', 0)}%">
            {text}
        </prosody>
    </voice>
</speak>"""

                result = synthesizer.speak_ssml_async(ssml).get()

                if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                    _normalize_audio_format(
                        str(temp_azure_output),
                        str(segment_file_raw),
                        sample_rate=CACHE_SAMPLE_RATE,
                        channels=CACHE_CHANNELS,
                    )
                    _write_cache_atomic(cache_file, segment_file_raw)
                    temp_azure_output.unlink(missing_ok=True)
                else:
                    raise RuntimeError(f"TTS failed: {result.reason}")

            # Get raw duration
            raw_ms = _get_duration_ms(str(segment_file_raw))

            # Trim silence
            trimmed_file = temp_path / f"seg_{utt_id}_trimmed.wav"
            trimmed_sec, saved_ms = _trim_silence(str(segment_file_raw), str(trimmed_file))
            trimmed_ms = int(trimmed_sec * 1000)

            # Determine rate and status
            if trimmed_ms <= budget_ms:
                # Fits within budget - pad to exact budget
                _pad_audio(str(trimmed_file), str(segment_file), budget_ms)
                final_ms = budget_ms
                rate = 1.0
                status = TTSSegmentStatus.SUCCESS
            else:
                # Need rate adjustment
                rate = trimmed_ms / budget_ms
                if rate <= max_rate:
                    # Safe rate adjustment
                    _apply_rate_and_pad(str(trimmed_file), str(segment_file), rate, budget_ms)
                    final_ms = budget_ms
                    status = TTSSegmentStatus.RATE_ADJUSTED
                elif allow_extend_ms > 0:
                    # Try with extension
                    extended_budget = budget_ms + allow_extend_ms
                    rate = trimmed_ms / extended_budget
                    if rate <= max_rate:
                        _apply_rate_and_pad(str(trimmed_file), str(segment_file), rate, extended_budget)
                        final_ms = extended_budget
                        status = TTSSegmentStatus.EXTENDED
                    else:
                        # Still too fast - fail fast
                        raise RuntimeError(
                            f"Cannot fit: {trimmed_ms}ms > {extended_budget}ms even at {max_rate}x rate"
                        )
                else:
                    # Fail fast - cannot fit
                    raise RuntimeError(
                        f"Cannot fit: {trimmed_ms}ms > {budget_ms}ms, would need {rate:.2f}x rate (max: {max_rate}x)"
                    )

            # Cleanup temp files
            trimmed_file.unlink(missing_ok=True)
            segment_file_raw.unlink(missing_ok=True)

            segment_reports.append(
                TTSSegmentReport(
                    utt_id=utt_id,
                    budget_ms=budget_ms,
                    raw_ms=raw_ms,
                    trimmed_ms=trimmed_ms,
                    final_ms=final_ms,
                    rate=rate,
                    status=status,
                    output_path=str(segment_file.relative_to(output_dir.parent)),
                )
            )
            print(f"  âœ… [{utt_id}] {raw_ms}ms â†’ {trimmed_ms}ms â†’ {final_ms}ms (rate={rate:.2f}x)")

        except Exception as e:
            # Record failure
            segment_reports.append(
                TTSSegmentReport(
                    utt_id=utt_id,
                    budget_ms=budget_ms,
                    raw_ms=0,
                    trimmed_ms=0,
                    final_ms=0,
                    rate=1.0,
                    status=TTSSegmentStatus.FAILED,
                    output_path="",
                    error=str(e),
                )
            )
            print(f"  âŒ [{utt_id}] Failed: {e}")

    return TTSReport(
        audio_duration_ms=dub_manifest.audio_duration_ms,
        segments_dir=segments_dir,
        segments=segment_reports,
    )


def _get_duration_ms(audio_path: str) -> int:
    """Get audio duration in milliseconds using ffprobe."""
    result = subprocess.run(
        [
            "ffprobe", "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            audio_path,
        ],
        capture_output=True,
        text=True,
    )
    duration_str = result.stdout.strip()
    if duration_str == "N/A" or not duration_str:
        return 0
    return int(float(duration_str) * 1000)


def _pad_audio(input_path: str, output_path: str, target_ms: int):
    """Pad audio to exact target duration."""
    current_ms = _get_duration_ms(input_path)
    if current_ms >= target_ms:
        shutil.copy2(input_path, output_path)
        return

    pad_duration_sec = (target_ms - current_ms) / 1000.0
    cmd = [
        "ffmpeg",
        "-i", input_path,
        "-af", f"apad=pad_dur={pad_duration_sec}",
        "-ar", str(CACHE_SAMPLE_RATE),
        "-ac", str(CACHE_CHANNELS),
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True, capture_output=True)


def _apply_rate_and_pad(input_path: str, output_path: str, rate: float, target_ms: int):
    """Apply tempo rate adjustment and pad to target duration."""
    # Build atempo filter chain (supports 0.5-2.0 range)
    if rate > 2.0:
        ratios = []
        remaining = rate
        while remaining > 2.0:
            ratios.append(2.0)
            remaining /= 2.0
        ratios.append(remaining)
        filter_str = ",".join([f"atempo={r}" for r in ratios])
    elif rate < 0.5:
        ratios = []
        remaining = rate
        while remaining < 0.5:
            ratios.append(0.5)
            remaining /= 0.5
        ratios.append(remaining)
        filter_str = ",".join([f"atempo={r}" for r in ratios])
    else:
        filter_str = f"atempo={rate}"

    # Apply rate, then pad to exact duration
    target_sec = target_ms / 1000.0
    cmd = [
        "ffmpeg",
        "-i", input_path,
        "-af", f"{filter_str},apad=whole_dur={target_sec}",
        "-t", str(target_sec),
        "-ar", str(CACHE_SAMPLE_RATE),
        "-ac", str(CACHE_CHANNELS),
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True, capture_output=True)
