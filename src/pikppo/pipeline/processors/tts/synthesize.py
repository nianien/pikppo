"""
æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å—ã€‚
å°†è‹±æ–‡å­—å¹•è½¬æ¢ä¸ºè¯­éŸ³ï¼Œå¹¶æ ¹æ®æ—¶é—´æˆ³åˆæˆéŸ³é¢‘ã€‚
"""
import hashlib
import os
import subprocess
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Optional

from pikppo.config.settings import PipelineConfig, get_openai_key
from pikppo.pipeline.processors.srt.srt import parse_srt
from pikppo.schema import SrtCue


def synthesize_subtitle_to_audio(
    subtitle_path: str,
    *,
    output_path: Optional[str] = None,
    video_path: Optional[str] = None,
    config: Optional[PipelineConfig] = None,
    voice: str = "alloy",
    speed: float = 1.0,
    replace_audio: bool = False,
) -> Dict[str, str]:
    """
    å°†è‹±æ–‡å­—å¹•æ–‡ä»¶è½¬æ¢ä¸ºè¯­éŸ³ï¼Œå¹¶åˆæˆéŸ³é¢‘ã€‚
    
    å‚æ•°:
        subtitle_path: è‹±æ–‡å­—å¹•æ–‡ä»¶è·¯å¾„ï¼ˆSRTï¼‰
        output_path: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š{subtitle}_audio.wavï¼‰
        video_path: åŸå§‹è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå¯ä»¥æ›¿æ¢æˆ–æ··åˆéŸ³é¢‘ï¼‰
        config: PipelineConfig
        voice: TTS è¯­éŸ³ï¼ˆOpenAI TTS: alloy, echo, fable, onyx, nova, shimmerï¼‰
        speed: è¯­éŸ³é€Ÿåº¦ï¼ˆ0.25-4.0ï¼Œé»˜è®¤ 1.0ï¼‰
        replace_audio: å¦‚æœæä¾› video_pathï¼Œæ˜¯å¦æ›¿æ¢åŸéŸ³é¢‘ï¼ˆTrueï¼‰æˆ–æ··åˆï¼ˆFalseï¼‰
    
    è¿”å›:
        {"audio": "<éŸ³é¢‘æ–‡ä»¶è·¯å¾„>", "video": "<è§†é¢‘æ–‡ä»¶è·¯å¾„>"ï¼ˆå¦‚æœæä¾›ï¼‰}
    """
    cfg = config or PipelineConfig()
    
    subtitle = Path(subtitle_path)
    if not subtitle.exists():
        raise FileNotFoundError(f"Subtitle file not found: {subtitle}")
    
    # è§£æå­—å¹•
    cues = parse_srt(subtitle)  # è¿”å› List[SrtCue]
    if not cues:
        raise RuntimeError(f"No segments found in subtitle file: {subtitle}")
    
    # è¾“å‡ºè·¯å¾„ï¼šç¡®ä¿ä¸è¾“å…¥æ–‡ä»¶åœ¨åŒä¸€ç›®å½•
    if output_path is None:
        # é»˜è®¤è¾“å‡ºåˆ°è¾“å…¥æ–‡ä»¶åŒä¸€ç›®å½•
        output_audio = subtitle.parent / f"{subtitle.stem}_audio.wav"
    else:
        output_audio = Path(output_path)
        # å¦‚æœè¾“å‡ºè·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œåˆ™ç›¸å¯¹äºè¾“å…¥æ–‡ä»¶ç›®å½•
        if not output_audio.is_absolute():
            output_audio = subtitle.parent / output_audio
    output_audio.parent.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("å­—å¹•è½¬è¯­éŸ³ï¼ˆTTSï¼‰")
    print("=" * 60)
    print(f"å­—å¹•æ–‡ä»¶: {subtitle}")
    print(f"è¾“å‡ºéŸ³é¢‘: {output_audio}")
    print(f"è¯­éŸ³: {voice}, é€Ÿåº¦: {speed}x")
    print(f"å­—å¹•ç‰‡æ®µæ•°: {len(cues)}")
    print("=" * 60)
    print()
    
    # æ£€æŸ¥ OpenAI API Key
    openai_key = get_openai_key()
    if not openai_key:
        raise RuntimeError(
            "OPENAI_KEY or OPENAI_API_KEY not found in environment variables. "
            "Please set it to use TTS."
        )
    
    # ä½¿ç”¨ OpenAI TTS åˆæˆè¯­éŸ³ï¼ˆå¹¶å‘å¤„ç†ï¼‰
    print("1. åˆæˆè¯­éŸ³ç‰‡æ®µï¼ˆå¹¶å‘å¤„ç†ï¼‰...")
    audio_segments = []
    
    # å‡†å¤‡ç¼“å­˜ç›®å½•ï¼ˆç”¨äº TTS ç‰‡æ®µç¼“å­˜ï¼‰
    cache_dir = subtitle.parent / ".cache" / "tts"
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir_path = Path(tmpdir)
        
        # å‡†å¤‡éœ€è¦åˆæˆçš„ä»»åŠ¡
        tasks = []
        for i, cue in enumerate(cues):
            # SrtCue å¯¹è±¡æœ‰ text å±æ€§
            text = cue.text.strip()
            if not text:
                continue
            
            # SrtCue ä½¿ç”¨æ¯«ç§’ï¼Œè½¬æ¢ä¸ºç§’
            start_time = cue.start_ms / 1000.0
            end_time = cue.end_ms / 1000.0
            duration = end_time - start_time
            
            # æ£€æŸ¥ TTS ç¼“å­˜
            cache_key = hashlib.md5(f"{text}_{voice}_{speed}".encode()).hexdigest()
            cached_segment = cache_dir / f"{cache_key}.mp3"
            
            if cached_segment.exists():
                # ä½¿ç”¨ç¼“å­˜
                audio_segments.append({
                    "path": cached_segment,
                    "start": start_time,
                    "duration": duration,
                })
                print(f"   ğŸ’¾ [{i+1}/{len(segments)}] ç¼“å­˜å‘½ä¸­: {text[:50]}...")
            else:
                # éœ€è¦åˆæˆ
                segment_audio = tmpdir_path / f"segment_{i:04d}.mp3"
                tasks.append({
                    "index": i,
                    "total": len(segments),
                    "text": text,
                    "output_path": str(segment_audio),
                    "cache_path": cached_segment,
                    "start": start_time,
                    "duration": duration,
                })
        
        # å¹¶å‘åˆæˆï¼ˆé»˜è®¤ 4 ä¸ª workerï¼Œå¯æ ¹æ®ç½‘ç»œ/é…é¢è°ƒæ•´ï¼‰
        max_workers = getattr(cfg, "tts_max_workers", 4)
        if tasks:
            print(f"   ğŸš€ å¹¶å‘åˆæˆ {len(tasks)} ä¸ªç‰‡æ®µï¼ˆworkers={max_workers}ï¼‰...")
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(
                        _synthesize_text_openai,
                        text=task["text"],
                        output_path=task["output_path"],
                        voice=voice,
                        speed=speed,
                        api_key=openai_key,
                    ): task
                    for task in tasks
                }
                
                for future in as_completed(futures):
                    task = futures[future]
                    try:
                        future.result()  # ç­‰å¾…å®Œæˆ
                        # å¤åˆ¶åˆ°ç¼“å­˜
                        import shutil
                        shutil.copy(task["output_path"], task["cache_path"])
                        
                        audio_segments.append({
                            "path": Path(task["output_path"]),
                            "start": task["start"],
                            "duration": task["duration"],
                        })
                        print(f"   âœ… [{task['index']+1}/{task['total']}] {task['text'][:50]}...")
                    except Exception as e:
                        print(f"   âš ï¸  [{task['index']+1}/{task['total']}] å¤±è´¥: {e}")
                        continue
        
        if not audio_segments:
            raise RuntimeError("No audio segments generated. Check TTS API key and network.")
        
        print()
        print(f"2. åˆæˆå®Œæ•´éŸ³é¢‘ï¼ˆå…± {len(audio_segments)} ä¸ªç‰‡æ®µï¼‰...")
        
        # å…ˆåˆæˆåŸå§‹éŸ³é¢‘ï¼ˆæœªå½’ä¸€åŒ–ï¼‰
        raw_audio = tmpdir_path / "audio_raw.wav"
        _concatenate_audio_segments(
            audio_segments=audio_segments,
            output_path=str(raw_audio),
        )
        print(f"   âœ… åŸå§‹éŸ³é¢‘å·²åˆæˆ")
        
        # 3. å“åº¦å½’ä¸€åŒ–ï¼ˆå…³é”®æ­¥éª¤ï¼šè§£å†³å£°éŸ³å°çš„é—®é¢˜ï¼‰
        print()
        print("3. å“åº¦å½’ä¸€åŒ–ï¼ˆLoudness Normalizationï¼‰...")
        target_lufs = getattr(cfg, "tts_target_lufs", -16.0)  # é»˜è®¤ -16 LUFSï¼ˆTikTok/çŸ­å‰§æ¨èï¼‰
        normalize_tts_audio(
            input_path=str(raw_audio),
            output_path=str(output_audio),
            target_lufs=target_lufs,
        )
        print(f"   âœ… å“åº¦å½’ä¸€åŒ–å®Œæˆï¼ˆç›®æ ‡: {target_lufs} LUFSï¼‰")
    
    result: Dict[str, str] = {"audio": str(output_audio)}
    
    # å¦‚æœæä¾›äº†è§†é¢‘è·¯å¾„ï¼Œæ›¿æ¢æˆ–æ··åˆéŸ³é¢‘
    # è¾“å‡ºè§†é¢‘ä¸è¾“å…¥å­—å¹•åœ¨åŒä¸€ç›®å½•ï¼ˆè€Œä¸æ˜¯è§†é¢‘ç›®å½•ï¼‰
    if video_path:
        video = Path(video_path)
        if not video.exists():
            raise FileNotFoundError(f"Video file not found: {video}")
        
        print()
        print("4. æ›¿æ¢/æ··åˆè§†é¢‘éŸ³é¢‘...")
        # è¾“å‡ºè§†é¢‘ä¸å­—å¹•æ–‡ä»¶åœ¨åŒä¸€ç›®å½•
        output_video = subtitle.parent / f"{subtitle.stem}_dubbed.mp4"
        
        if replace_audio or getattr(cfg, "tts_mute_original", False):
            # å®Œå…¨æ›¿æ¢éŸ³é¢‘ï¼ˆé™éŸ³åŸéŸ³é¢‘ï¼‰
            if getattr(cfg, "tts_mute_original", False):
                print("   ğŸ”‡ å®Œå…¨é™éŸ³åŸéŸ³é¢‘æ¨¡å¼ï¼šåªä¿ç•™è‹±æ–‡é…éŸ³")
            else:
                print("   âš ï¸  å®Œå…¨æ›¿æ¢éŸ³é¢‘æ¨¡å¼ï¼šå°†ä¸¢å¤±åŸè§†é¢‘ BGM/ç¯å¢ƒå£°")
            _replace_video_audio(str(video), str(output_audio), str(output_video))
        else:
            # ä¸“ä¸šæ··éŸ³ï¼ˆæ¨èï¼‰ï¼šä½¿ç”¨ä¾§é“¾å‹ç¼©ï¼Œä¿ç•™åŸéŸ³æ°›å›´
            mix_mode = getattr(cfg, "tts_mix_mode", "ducking")
            scene_type = getattr(cfg, "tts_scene_type", "normal")
            
            if mix_mode == "ducking":
                # ä½¿ç”¨ä¸“ä¸šçš„ä¾§é“¾å‹ç¼©æ··éŸ³
                mix_video_audio_with_ducking(
                    str(video),
                    str(output_audio),
                    str(output_video),
                    scene_type=scene_type,
                    tts_volume=getattr(cfg, "tts_volume", 1.4),
                )
            else:
                # ç®€å•æ··åˆï¼ˆä¿å®ˆæ–¹æ¡ˆï¼‰
                _mix_video_audio(
                    str(video),
                    str(output_audio),
                    str(output_video),
                    mix_mode="simple",
                    original_volume=0.4,
                    tts_volume=getattr(cfg, "tts_volume", 1.4),
                )
        
        print(f"   âœ… è§†é¢‘å·²ç”Ÿæˆ: {output_video}")
        result["video"] = str(output_video)
    
    print()
    print("=" * 60)
    print("âœ… å®Œæˆï¼")
    print("=" * 60)
    
    return result


def _synthesize_text_openai(
    text: str,
    output_path: str,
    voice: str = "alloy",
    speed: float = 1.0,
    api_key: Optional[str] = None,
) -> None:
    """ä½¿ç”¨ OpenAI TTS API åˆæˆè¯­éŸ³ã€‚"""
    try:
        from openai import OpenAI
    except ImportError:
        raise ImportError(
            "openai package is required for TTS. Install it with: pip install openai"
        )
    
    client = OpenAI(api_key=api_key)
    
    response = client.audio.speech.create(
        model="tts-1",  # æˆ– "tts-1-hd" (æ›´é«˜è´¨é‡ï¼Œæ›´æ…¢)
        voice=voice,
        input=text,
        speed=speed,
    )
    
    # ä¿å­˜éŸ³é¢‘
    response.stream_to_file(output_path)


def _concatenate_audio_segments(
    audio_segments: List[Dict],
    output_path: str,
) -> None:
    """
    å°†å¤šä¸ªéŸ³é¢‘ç‰‡æ®µæŒ‰æ—¶é—´æˆ³åˆæˆå®Œæ•´éŸ³é¢‘ã€‚
    
    å‚æ•°:
        audio_segments: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {"path": Path, "start": float, "duration": float}
        output_path: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„
    """
    if not audio_segments:
        raise ValueError("No audio segments to concatenate")
    
    # è®¡ç®—æ€»æ—¶é•¿
    max_end = max(seg["start"] + seg["duration"] for seg in audio_segments)
    total_duration = max_end
    
    # ç”Ÿæˆ ffmpeg filter_complex
    # ä¸ºæ¯ä¸ªç‰‡æ®µåˆ›å»ºå»¶è¿Ÿå’Œå¡«å……
    delays = []
    mix_inputs = []
    
    for i, seg in enumerate(audio_segments):
        delay_ms = int(seg["start"] * 1000)
        # ä¸ºæ¯ä¸ªè¾“å…¥æ·»åŠ å»¶è¿Ÿ
        delays.append(f"[{i}]adelay={delay_ms}|{delay_ms}[a{i}]")
        mix_inputs.append(f"[a{i}]")
    
    # æ„å»º filter_complex
    if len(audio_segments) == 1:
        filter_complex = f"{delays[0]};[a0]volume=1.0[out]"
    else:
        # åˆå¹¶æ‰€æœ‰å»¶è¿Ÿåçš„éŸ³é¢‘æµ
        filter_complex = ";".join(delays) + f";{''.join(mix_inputs)}amix=inputs={len(audio_segments)}:duration=longest:dropout_transition=0[out]"
    
    # æ„å»º ffmpeg å‘½ä»¤
    cmd = ["ffmpeg", "-y"]
    
    # æ·»åŠ æ‰€æœ‰è¾“å…¥æ–‡ä»¶
    for seg in audio_segments:
        cmd.extend(["-i", str(seg["path"])])
    
    # æ·»åŠ  filter_complex
    cmd.extend(["-filter_complex", filter_complex])
    
    # è¾“å‡ºè®¾ç½®
    cmd.extend([
        "-map", "[out]",
        "-ar", "44100",
        "-ac", "2",
        "-acodec", "pcm_s16le",
        output_path,
    ])
    
    subprocess.run(cmd, check=True, capture_output=True)


def _replace_video_audio(video_path: str, audio_path: str, output_path: str) -> None:
    """æ›¿æ¢è§†é¢‘çš„éŸ³é¢‘è½¨é“ã€‚"""
    cmd = [
        "ffmpeg",
        "-i", video_path,
        "-i", audio_path,
        "-c:v", "copy",
        "-c:a", "aac",
        "-map", "0:v:0",
        "-map", "1:a:0",
        "-shortest",
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True)


def normalize_tts_audio(
    input_path: str,
    output_path: str,
    target_lufs: float = -16.0,
) -> None:
    """
    å¯¹ TTS éŸ³é¢‘è¿›è¡Œå“åº¦å½’ä¸€åŒ–ï¼ˆLoudness Normalizationï¼‰ã€‚
    
    è¿™æ˜¯è§£å†³ TTS å£°éŸ³å°é—®é¢˜çš„å…³é”®æ­¥éª¤ã€‚
    TTS æœåŠ¡é»˜è®¤è¾“å‡ºä¿å®ˆç”µå¹³ï¼ˆé€šå¸¸ -22 ~ -24 LUFSï¼‰ï¼Œ
    éœ€è¦å½’ä¸€åŒ–åˆ°å¹³å°æ ‡å‡†ï¼ˆ-16 LUFS é€‚åˆ TikTok/çŸ­å‰§ï¼‰ã€‚
    
    å‚æ•°:
        input_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶ï¼ˆåŸå§‹ TTS è¾“å‡ºï¼‰
        output_path: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶ï¼ˆå½’ä¸€åŒ–åï¼‰
        target_lufs: ç›®æ ‡å“åº¦ï¼ˆLUFSï¼‰
            - -16 LUFS: TikTok/çŸ­å‰§æ¨è
            - -14 LUFS: YouTube/Shorts
            - -23 LUFS: å¹¿æ’­æ ‡å‡†
    
    æ³¨æ„:
        - loudnorm å¿…é¡»åœ¨æœ€ååšï¼ˆå¯¹æ•´æ¡éŸ³è½¨ï¼‰
        - ä¸è¦åœ¨æ¯å¥å•ç‹¬åšï¼Œä¼šç ´åä¸€è‡´æ€§
    """
    cmd = [
        "ffmpeg",
        "-i", input_path,
        "-af", f"loudnorm=I={target_lufs}:TP=-1.5:LRA=11",
        "-c:a", "pcm_s16le",
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True, capture_output=True)


def _mix_video_audio(
    video_path: str,
    audio_path: str,
    output_path: str,
    mix_mode: str = "ducking",
    original_volume: float = 1.0,
    tts_volume: float = 1.4,
) -> None:
    """
    æ··åˆè§†é¢‘åŸéŸ³é¢‘å’Œæ–°çš„ TTS éŸ³é¢‘ï¼ˆä¸“ä¸šæ··éŸ³ï¼‰ã€‚
    
    æ”¯æŒä¸¤ç§æ··éŸ³æ¨¡å¼ï¼š
    1. duckingï¼ˆæ¨èï¼‰ï¼šä¾§é“¾å‹ç¼©ï¼Œå½“ TTS å“èµ·æ—¶è‡ªåŠ¨å‹ä½åŸéŸ³é¢‘
    2. simpleï¼šç®€å•æ··åˆï¼Œå›ºå®šå‹ä½åŸéŸ³é¢‘
    
    å‚æ•°:
        video_path: åŸå§‹è§†é¢‘æ–‡ä»¶
        audio_path: TTS éŸ³é¢‘æ–‡ä»¶ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
        output_path: è¾“å‡ºè§†é¢‘æ–‡ä»¶
        mix_mode: æ··éŸ³æ¨¡å¼ï¼ˆ"ducking" æˆ– "simple"ï¼‰
        original_volume: åŸéŸ³é¢‘åŸºç¡€éŸ³é‡ï¼ˆducking æ¨¡å¼ä¸‹ä¸ºå³°å€¼ï¼Œsimple æ¨¡å¼ä¸‹ä¸ºå›ºå®šå€¼ï¼‰
        tts_volume: TTS éŸ³é¢‘éŸ³é‡ï¼ˆé»˜è®¤ 1.4ï¼Œç¨å¾®æ”¾å¤§ç¡®ä¿æ¸…æ™°ï¼‰
    """
    if mix_mode == "ducking":
        # ä¾§é“¾å‹ç¼©ï¼ˆSidechain Duckingï¼‰- ä½¿ç”¨ acompressor å®ç°
        # å½“è‹±æ–‡é…éŸ³å‡ºç°æ—¶ï¼Œè‡ªåŠ¨å‹ä½åŸè§†é¢‘éŸ³é‡ï¼›é…éŸ³ç»“æŸåï¼Œè‡ªåŠ¨æ¢å¤
        # ä½¿ç”¨ acompressor çš„ä¾§é“¾åŠŸèƒ½ï¼šç”¨ TTS éŸ³é¢‘ä½œä¸ºä¾§é“¾ä¿¡å·æ§åˆ¶åŸéŸ³é¢‘
        filter_complex = (
            f"[0:a]acompressor=threshold=0.02:ratio=10:attack=20:release=500:makeup=1.0:detection=peak:link=1[a0_compressed];"
            f"[1:a]volume={tts_volume}[a1];"
            f"[a0_compressed][a1]amix=inputs=2:weights=1 3:duration=longest[mix]"
        )
    else:
        # ç®€å•æ··åˆï¼ˆä¿å®ˆæ–¹æ¡ˆï¼‰
        # å…¨ç¨‹å¤§å¹…å‹ä½åŸéŸ³é¢‘ï¼Œç¡®ä¿è‹±æ–‡é…éŸ³æ¸…æ™°
        filter_complex = (
            f"[0:a]volume={original_volume * 0.2}[a0];"  # åŸéŸ³é¢‘å‹ä½åˆ° 20%ï¼ˆæ›´æ¿€è¿›ï¼‰
            f"[1:a]volume={tts_volume}[a1];"
            f"[a0][a1]amix=inputs=2:weights=1 5:duration=longest[mix]"  # TTS æƒé‡ 5:1
        )
    
    cmd = [
        "ffmpeg",
        "-i", video_path,
        "-i", audio_path,
        "-filter_complex", filter_complex,
        "-map", "0:v:0",
        "-map", "[mix]",
        "-c:v", "copy",
        "-c:a", "aac",
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True)


def mix_video_audio_with_ducking(
    video_path: str,
    audio_path: str,
    output_path: str,
    *,
    scene_type: str = "normal",
    tts_volume: float = 1.4,
) -> None:
    """
    ä¸“ä¸šçš„è§†é¢‘éŸ³é¢‘æ··éŸ³ï¼ˆå¸¦ä¾§é“¾å‹ç¼©ï¼‰ã€‚
    
    è¿™æ˜¯çŸ­å‰§å‡ºæµ·çš„æ ‡å‡†åšæ³•ï¼š
    - ä¿ç•™åŸè§†é¢‘ BGM / ç¯å¢ƒå£°
    - å½“è‹±æ–‡é…éŸ³å‡ºç°æ—¶ï¼Œè‡ªåŠ¨å‹ä½åŸè§†é¢‘éŸ³é‡ï¼ˆduckingï¼‰
    - é…éŸ³ç»“æŸåï¼Œè‡ªåŠ¨æ¢å¤åŸéŸ³é‡
    
    å‚æ•°:
        video_path: åŸå§‹è§†é¢‘æ–‡ä»¶
        audio_path: TTS éŸ³é¢‘æ–‡ä»¶ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
        output_path: è¾“å‡ºè§†é¢‘æ–‡ä»¶
        scene_type: åœºæ™¯ç±»å‹ï¼Œå½±å“æ··éŸ³å‚æ•°
            - "quiet": å®‰é™åœºæ™¯ï¼ˆæ›´æ¿€è¿›çš„ duckingï¼‰
            - "normal": æ­£å¸¸åœºæ™¯ï¼ˆé»˜è®¤ï¼‰
            - "intense": æ¿€çƒˆåœºæ™¯ï¼ˆä¿ç•™æ›´å¤šåŸéŸ³æ°›å›´ï¼‰
        tts_volume: TTS éŸ³é¢‘éŸ³é‡ï¼ˆé»˜è®¤ 1.4ï¼‰
    
    æ··éŸ³ç­–ç•¥ï¼š
    - quiet: æ›´ä½çš„ thresholdï¼Œæ›´å¿«çš„ attackï¼Œç¡®ä¿é…éŸ³æ¸…æ™°
    - normal: å¹³è¡¡çš„ ducking å‚æ•°
    - intense: æ›´é«˜çš„ thresholdï¼Œä¿ç•™æ›´å¤šåŸéŸ³æ°›å›´
    """
    # æ ¹æ®åœºæ™¯ç±»å‹è°ƒæ•´ ducking å‚æ•°
    ducking_params = {
        "quiet": {
            "threshold": 0.015,  # æ›´æ•æ„Ÿ
            "ratio": 12,  # æ›´å¼ºçš„å‹ç¼©
            "attack": 15,  # æ›´å¿«çš„å“åº”
            "release": 400,  # æ›´å¿«çš„æ¢å¤
        },
        "normal": {
            "threshold": 0.02,
            "ratio": 10,
            "attack": 20,
            "release": 500,
        },
        "intense": {
            "threshold": 0.03,  # æ›´ä¸æ•æ„Ÿï¼Œä¿ç•™æ›´å¤šåŸéŸ³
            "ratio": 8,  # æ›´æ¸©å’Œçš„å‹ç¼©
            "attack": 30,  # ç¨æ…¢çš„å“åº”
            "release": 600,  # æ›´æ…¢çš„æ¢å¤
        },
    }
    
    params = ducking_params.get(scene_type, ducking_params["normal"])
    
    # ä½¿ç”¨ä¾§é“¾å‹ç¼©ï¼ˆsidechaincompressï¼‰ï¼šç”¨ TTS éŸ³é¢‘ä½œä¸ºä¾§é“¾ä¿¡å·æ§åˆ¶åŸéŸ³é¢‘
    # å½“ TTS å“èµ·æ—¶ï¼Œè‡ªåŠ¨å‹ä½åŸéŸ³é¢‘ï¼›TTS ç»“æŸåï¼Œè‡ªåŠ¨æ¢å¤
    # makeup å‚æ•°èŒƒå›´æ˜¯ 1-64ï¼Œ1 è¡¨ç¤ºä¸å¢ç›Šï¼Œæ›´å¤§çš„å€¼è¡¨ç¤ºå¢ç›Š
    # æˆ‘ä»¬ä½¿ç”¨ 1.0ï¼ˆæœ€å°å¢ç›Šï¼‰æ¥å‹ä½åŸéŸ³é¢‘ï¼Œç„¶åé€šè¿‡ volume è¿›ä¸€æ­¥æ§åˆ¶
    filter_complex = (
        f"[0:a][1:a]sidechaincompress="
        f"threshold={params['threshold']}:"
        f"ratio={params['ratio']}:"
        f"attack={params['attack']}:"
        f"release={params['release']}:"
        f"makeup=1.0[a0_compressed];"  # makeup=1.0ï¼ˆæœ€å°å¢ç›Šï¼Œé…åˆ volume å‹ä½ï¼‰
        f"[a0_compressed]volume=0.2[a0_low];"  # è¿›ä¸€æ­¥å‹ä½åŸéŸ³é¢‘åˆ° 20%
        f"[1:a]volume={tts_volume}[a1];"
        f"[a0_low][a1]amix=inputs=2:weights=1 5:duration=longest[mix]"  # TTS æƒé‡ 5:1
    )
    
    cmd = [
        "ffmpeg",
        "-i", video_path,
        "-i", audio_path,
        "-filter_complex", filter_complex,
        "-map", "0:v:0",
        "-map", "[mix]",
        "-c:v", "copy",
        "-c:a", "aac",
        "-y",
        output_path,
    ]
    subprocess.run(cmd, check=True)
