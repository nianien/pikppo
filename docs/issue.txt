下面我按“现状诊断 → 根因定位到具体文件/逻辑 → 立刻可落地的整改方案（v1）”给你一份确定性的改造清单。

⸻

你现有代码做得好的地方（保留不动）
	1.	GCS 规范化是对的
pipeline/asr_google.py 已经锁定了 bucket + 路径规范（你定的 pikppo-asr-audio，gs://{bucket}/asr/{stem}/audio.wav），这能避免后续路径乱套。
	2.	ASR 输出结构正确
Google STT 的输出被聚合成 segment：{id,start,end,text,speaker}（asr_google.py::_aggregate_to_segments()），这对“翻译、分配声线、按段合成”是正确的接口契约。
	3.	翻译阶段保留时间戳
pipeline/translate_openai.py 生成的 en_segments.json 仍然包含 start/end/speaker，说明翻译链路不会破坏时间轴（问题不在翻译）。
	4.	你“对白被压制”这点确实是有效的结果
pipeline/mix_audio.py 用 sidechaincompress 让 BGM/伴奏给英文 TTS 让路，这个方向没问题（只是后面 timing 和 voice 搞崩了）。

⸻

现在三大问题分别“根因在哪”

问题 1：字幕时间完全对不上

根因不是“字幕生成”，而是你生成的英文音频时间轴从一开始就不对，字幕当然跟不上。

核心原因有两条（都在 tts_azure.py 的拼接策略）：

A. 你把所有 segment 的音频“直接无缝 concat”了，完全丢掉了原始时间轴的空隙（gap）
tts_azure.py::_concatenate_audio_files() 是按 seg_0001.wav, seg_0002.wav... 直接拼接。
但真实时间轴是：seg1.end -> seg2.start 中间经常有停顿/无对白/转场。你没把这些 gap 插回去，于是音频会整体“越跑越提前”，字幕就必然越来越错。

B. Step9 的 duration_align.py 用“整段 time-stretch”去强行贴总时长，会把段内对齐彻底破坏
你现在是：TTS 先拼成一整条 → 再用 align_duration() 按总时长做 atempo。
这会让每个字的相对位置漂移，字幕跟口型/语义点全部失配。

结论：字幕时间对不上不是“字幕文件写错了”，是你“音频时间轴构建错了”。

⸻

问题 2：人物音色完全不对

根因基本就在 pipeline/assign_voices.py：
	1.	如果没有 librosa / pitch 检测失败，你的 gender 基本全是 unknown
unknown 在你的逻辑里会大量落到 female fallback（Ana/Jenny 之类），结果就是“男的也女声，女的也女声”，你就觉得“完全不对”。
	2.	你的性别检测是在“整条 vocals.wav”上截取时间片
这一步理论上可行，但在短剧场景（BGM大、混响、多人、笑/哭/吼）里，pyin 很容易失败或误判。
	3.	你当前只做了“speaker diarization → speaker_id → voice_pool”映射
没有“声纹聚类/说话人重识别”的稳定机制，但 v1 你也明确说只要“声线池（男女不同像配音员）”，那就别把系统复杂度押在 pitch 检测上。

⸻

问题 3：没有英文字幕

这个是“产物存在但没进视频”：

在 pipeline/dub_pipeline.py Step6 已经生成了：
	•	dub/<stem>/<stem>-en.srt
	•	dub/<stem>/<stem>-en.ass

但 Step10 的 mix_audio.py 只做了音频混音并输出 mp4，没有烧字幕。
所以你看到的视频“没有英文字幕”，不是没生成，是没合进去。

⸻

v1 完整整改方案（按你标准：效果 > 在线模型优先 > 便捷 > 统一）

下面这套改完，三件事会同时解决：字幕对齐、声线合理、视频带英文字幕。

⸻

1) 重做 TTS 音频时间轴（这是最关键的整改）

目标：生成的英文配音音频必须严格遵守原 segment 时间轴：
	•	第一段前面补 0 -> seg1.start 的静音
	•	段与段之间补 prev.end -> next.start 的静音
	•	每个 segment 的语音长度要被控制在 seg.end - seg.start 的窗口内
	•	不再做“整段 duration_align”

具体改法（你现有结构最小改动）

修改 pipeline/tts_azure.py：

(1) 合成每个 seg 后，立刻做“段内对齐”
	•	用 ffprobe 获取该 seg 生成的 wav 时长 tts_len
	•	目标时长 target = seg.end - seg.start

策略：
	•	如果 tts_len < target：在 seg 末尾 pad 静音补齐（apad + atrim）
	•	如果 tts_len > target：
	•	先尝试 atempo 压缩到 target（限制最大压缩比，比如 1.25 / 1.35）
	•	如果超出阈值：回到翻译阶段做更短文案（你翻译 prompt 已经有 CPS 约束，但实际还会超，需要加一轮“为配音压缩”）

(2) 在 concat 前插入 gap 静音段
	•	遍历 en_segments（按 start 排序）
	•	gap = seg.start - prev_end
	•	gap > 0 就生成一段静音 wav，插入 concat list

这样拼出来的整条 tts 音频就天然和时间轴对齐了。

(3) 删除 Step9 的 duration_align（或变成 no-op）
pipeline/dub_pipeline.py：
	•	Step9 不要再调用 align_duration()
	•	或者 align_duration() 改为只做“最后总长度微调（±0.2s）”，但通常不需要。

这一条改完，你说的“字幕时间完全对不上”会直接被解决 80%+。

⸻

2) 声线分配改成“稳定且符合 v1 目标”的策略（别赌 pitch）

你自己已经明确了 v1 目标：声线池就够（男女不同，像配音员）。
那就用“稳定的工程策略”，别用脆弱的 pitch 推断去赌。

具体改法

修改 pipeline/assign_voices.py：
	•	不依赖 librosa 做性别检测（可以保留作为“可选增强”）
	•	默认策略改为：

A. 先按 speaker 总对白时长排序（你现在已有）
取 topK（你已有动态 k）

B. 给 topK 做“交替分配”
	•	speaker rank 1 → male_1
	•	rank 2 → female_1
	•	rank 3 → male_2
	•	rank 4 → female_2
（你的 voice pool 已经是 8 条美音池，非常适合）

C. 其余 speaker 全部用固定“群众声线”
比如统一用 1 男 1 女随机（但对同一 speaker 要稳定：hash speaker_id）。

这样你能保证：
	•	同一 speaker 在一集内永远同一个 voice
	•	男女性别至少“听感上分开”
	•	不会出现“全员女声”的灾难

你要的不是 Vozo 那种真声纹克隆，这个策略是 v1 最稳、最便宜、最不脆弱的工程解。

⸻

3) 英文字幕“可见化”：输出视频必须带字幕

你有三种方式，v1 我建议默认给你“两份输出”：

输出 A：带英文字幕的成片（硬烧）

在 dub_pipeline.py Step10 之后加一步：
	•	输入：<stem>-dubbed.mp4 + <stem>-en.ass
	•	输出：<stem>-dubbed-subbed.mp4

ffmpeg 典型做法就是 -vf ass=...（你工程里已经有 ffmpeg 依赖）。

输出 B：外挂字幕（软字幕）

把 1-en.srt / 1-en.ass 保留并打印路径，播放器/剪辑软件可直接加载。

你现在“没有英文字幕”只是因为没烧进去，不是你没生成。把这一步补上就完事。

⸻

4) 为什么你现在觉得“对白被压制住了”但整体效果仍然差？

因为你现在的混音是对的（sidechain 思路正确），但：
	•	TTS 语音 时间轴跑偏 → 该说话的时候没说 → 你听起来像“对白被压制/断断续续/错位”
	•	声线映射 不稳定 → 同一个人一会男一会女 → 观感灾难

所以你感知到“压制对白”不是要解决的问题，而是你“混音达成的效果”，但它被 timing/voice 问题掩盖了。

⸻

最终你会得到什么（改完的验收标准）

对每集 ≤3 分钟：
	1.	1-en.srt / 1-en.ass 的时间轴和“英文配音音频”一致
	2.	1-tts.wav（或 aligned）听感上跟画面节奏基本同步，转场/停顿不再跑偏
	3.	同一 speaker 全程同一声线，男女至少分开
	4.	输出视频至少有一个版本带英文字幕（硬烧）

⸻

--------------------